# Working with LLMs
Working with Ollama Models
Local llama 3.2

Install "Ollama"
Fetch Llama 3.2 {Ollama run llama3.2}
Then run the code
